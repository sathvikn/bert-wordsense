{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from core.analysis import access_db, get_senses, fb_to_local\n",
    "from core import semcor_bert_pipeline\n",
    "from core.metrics import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the shared words, we find that homonymous and polysemous senses are able to be distinguished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/bert/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1515: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  average, \"true nor predicted\", 'F-score is', len(true_sum)\n"
     ]
    }
   ],
   "source": [
    "shared_metrics = {}\n",
    "shared_words = ['foot.n', 'plane.n', 'model.n', 'table.n', 'degree.n', 'right.n']\n",
    "fmt_sense_12 = lambda w: [w + '.01', w + '.02']\n",
    "shared_senses = [fmt_sense_12(w) for w in shared_words]\n",
    "shared_senses[-1] = ['degree.n.01', 'academic_degree.n.01']\n",
    "shared_senses[1] = ['airplane.n.01', 'plane.n.02']\n",
    "for w_s in zip(shared_words, shared_senses):\n",
    "    model_data = binary_logistic(w_s[0], w_s[1])\n",
    "    weight_values, weight_indices = nonzero_weights(model_data['model'])\n",
    "    f_scores, accuracies, wrong_indices = k_fold_cv(model_data['data'], model_data['transformed_labels'], k = )\n",
    "    shared_metrics[w_s[0]] = {'senses': w_s[1], 'data': model_data, 'weights': weight_values,\n",
    "                    'weight_indices': weight_indices, 'f1_kfold': f_scores, 'acc_kfold': accuracies,\n",
    "                             'incorrect_indices': wrong_indices}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_type</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>avg_acc</th>\n",
       "      <th>pct_nonzero_weights</th>\n",
       "      <th>max_wt</th>\n",
       "      <th>min_wt</th>\n",
       "      <th>mean_nonzero_wt</th>\n",
       "      <th>sd_nonzero_wt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foot.n</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.221638</td>\n",
       "      <td>-0.229722</td>\n",
       "      <td>0.028953</td>\n",
       "      <td>0.132456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plane.n</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.491381</td>\n",
       "      <td>-0.265000</td>\n",
       "      <td>0.054483</td>\n",
       "      <td>0.223284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.n</td>\n",
       "      <td>0.518095</td>\n",
       "      <td>0.885455</td>\n",
       "      <td>0.037760</td>\n",
       "      <td>0.274155</td>\n",
       "      <td>-0.291541</td>\n",
       "      <td>0.020775</td>\n",
       "      <td>0.136917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table.n</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.033854</td>\n",
       "      <td>0.475794</td>\n",
       "      <td>-0.253788</td>\n",
       "      <td>0.013552</td>\n",
       "      <td>0.151044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>degree.n</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.018229</td>\n",
       "      <td>0.456742</td>\n",
       "      <td>-0.227727</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.166292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  word_type    avg_f1   avg_acc  pct_nonzero_weights    max_wt    min_wt  \\\n",
       "0    foot.n  1.000000  1.000000             0.031250  0.221638 -0.229722   \n",
       "1   plane.n  0.933333  0.980000             0.015625  0.491381 -0.265000   \n",
       "2   model.n  0.518095  0.885455             0.037760  0.274155 -0.291541   \n",
       "3   table.n  0.992000  0.993548             0.033854  0.475794 -0.253788   \n",
       "4  degree.n  1.000000  1.000000             0.018229  0.456742 -0.227727   \n",
       "\n",
       "   mean_nonzero_wt  sd_nonzero_wt  \n",
       "0         0.028953       0.132456  \n",
       "1         0.054483       0.223284  \n",
       "2         0.020775       0.136917  \n",
       "3         0.013552       0.151044  \n",
       "4         0.001208       0.166292  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_stats = []\n",
    "incorrect = []\n",
    "for k in shared_metrics:\n",
    "    word_results = shared_metrics[k]\n",
    "    weights = word_results['weights']\n",
    "    incorrect.append(misclassified_sentences(word_results['data'], word_results['incorrect_indices']))\n",
    "    shared_stats.append({'word_type': k, 'avg_f1': np.mean(word_results['f1_kfold']), 'avg_acc': np.mean(word_results['acc_kfold']),\n",
    "    'pct_nonzero_weights': len(weights) / 768, 'max_wt': max(weights), 'min_wt': min(weights), 'mean_nonzero_wt': np.mean(weights), 'sd_nonzero_wt': np.std(weights)})\n",
    "pd.DataFrame(shared_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Misclassified senses (Only binary classification, 3/16 senses for math plane were misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true_label</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>plane.n.02</td>\n",
       "      <td>The plane of the action in the scene is not parallel with the plane of the film in the camera or on the screen .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model.n.01</td>\n",
       "      <td>His first model arrived at dusk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model.n.01</td>\n",
       "      <td>We devote a chapter to the binomial distribution not only because it is a mathematical model for an enormous variety of real life phenomena , but also because it has important properties that recur in many other probability models .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.n.01</td>\n",
       "      <td>His first model arrived at dusk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model.n.01</td>\n",
       "      <td>His first model arrived at dusk .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>model.n.01</td>\n",
       "      <td>The model quite plainly thought Michelangelo crazy ; only the instructions from his rabbi kept him from bolting .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>model.n.01</td>\n",
       "      <td>But at the end of the sitting , when Michelangelo showed him the quick , free drawings , with the mother roughed in , holding her son , the model grasped what Michelangelo was after , and promised to speak to his friends .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>table.n.01</td>\n",
       "      <td>The registration figures given in Table 2 must be interpreted with caution since the estimate for eligible electors were made without the benefit of a reliable census .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   true_label  \\\n",
       "0  plane.n.02   \n",
       "0  model.n.01   \n",
       "1  model.n.01   \n",
       "2  model.n.01   \n",
       "3  model.n.01   \n",
       "4  model.n.01   \n",
       "5  model.n.01   \n",
       "0  table.n.01   \n",
       "\n",
       "                                                                                                                                                                                                                                  sentences  \n",
       "0                                                                                                                          The plane of the action in the scene is not parallel with the plane of the film in the camera or on the screen .  \n",
       "0                                                                                                                                                                                                         His first model arrived at dusk .  \n",
       "1  We devote a chapter to the binomial distribution not only because it is a mathematical model for an enormous variety of real life phenomena , but also because it has important properties that recur in many other probability models .  \n",
       "2                                                                                                                                                                                                         His first model arrived at dusk .  \n",
       "3                                                                                                                                                                                                         His first model arrived at dusk .  \n",
       "4                                                                                                                         The model quite plainly thought Michelangelo crazy ; only the instructions from his rabbi kept him from bolting .  \n",
       "5            But at the end of the sitting , when Michelangelo showed him the quick , free drawings , with the mother roughed in , holding her son , the model grasped what Michelangelo was after , and promised to speak to his friends .  \n",
       "0                                                                  The registration figures given in Table 2 must be interpreted with caution since the estimate for eligible electors were made without the benefit of a reliable census .  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 500)\n",
    "pd.concat(incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I thought this might be useful to see if weights at similar positions were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('foot.n',\n",
       "  array([ 13,  29,  40,  70,  89, 184, 191, 217, 231, 278, 287, 304, 308,\n",
       "         332, 493, 518, 528, 532, 547, 637, 664, 693, 709, 730])),\n",
       " ('plane.n',\n",
       "  array([105, 157, 254, 286, 308, 324, 411, 413, 448, 513, 539, 619])),\n",
       " ('model.n',\n",
       "  array([ 18,  49,  68,  74,  79, 101, 114, 242, 254, 259, 282, 289, 340,\n",
       "         364, 450, 473, 477, 509, 520, 523, 525, 527, 552, 574, 586, 597,\n",
       "         618, 739, 749])),\n",
       " ('table.n',\n",
       "  array([ 37,  67,  94, 136, 143, 188, 213, 228, 252, 286, 298, 317, 332,\n",
       "         334, 363, 376, 393, 445, 481, 487, 552, 661, 666, 680, 705, 753])),\n",
       " ('degree.n',\n",
       "  array([ 15,  22, 220, 262, 328, 350, 432, 523, 537, 541, 544, 620, 622,\n",
       "         724]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, shared_metrics[k]['weight_indices']) for k in shared_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cv(x, y, k = 5, labels = []):\n",
    "    kf = KFold(n_splits = k, shuffle = True)\n",
    "    f = []\n",
    "    acc = []\n",
    "    incorrect_indices = []\n",
    "    confusion_matrices = []\n",
    "    for train_index, test_index in kf.split(x):\n",
    "        model = LogisticRegression(penalty = 'l1', multi_class = 'multinomial', solver = 'saga', max_iter = 5000)\n",
    "        X_train, X_test = x[train_index], x[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train, y_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        #print(classification_report(y_test, test_pred))\n",
    "        f.append(f1_score(y_test, test_pred, average = \"weighted\"))\n",
    "        acc.append(accuracy_score(y_test, test_pred))\n",
    "        print(accuracy_score(y_test, test_pred))\n",
    "        print(y_test, test_pred)\n",
    "        print(confusion_matrix(y_test, test_pred))\n",
    "        confusion_matrices.append(confusion_matrix(y_test, test_pred))\n",
    "        incorrect_indices +=[i[0] for i in np.argwhere(y_test != test_pred)]\n",
    "    return f, acc, incorrect_indices, confusion_matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_cv(lemma, sel_senses = [], use_masc = True):\n",
    "    name, pos = lemma.split(\".\")\n",
    "    data = semcor_bert_pipeline.load_data(name, pos, 'semcor')\n",
    "    embeddings = data['embeddings']\n",
    "    sense_labels = data['sense_labels']\n",
    "    strip_synset = lambda s: s.strip(\"Synset()\").strip(\"'\")\n",
    "    target_senses = [strip_synset(i) for i in data['sense_names']]\n",
    "    try:\n",
    "        masc_data = semcor_bert_pipeline.load_data(name, pos, 'masc')\n",
    "        embeddings += masc_data['embeddings']\n",
    "        sense_labels += masc_data['sense_labels']\n",
    "    except:\n",
    "        pass\n",
    "    le = LabelEncoder()\n",
    "    le.fit(target_senses)\n",
    "    x = np.asarray(embeddings)\n",
    "    y = le.transform(sense_labels)\n",
    "    if len(sel_senses):\n",
    "        print(len(x))\n",
    "        sense_indices = [i for i in range(len(sense_labels)) if sense_labels[i] in sel_senses]\n",
    "        print(sense_indices)\n",
    "        x = x[sense_indices]\n",
    "        y = y[sense_indices]\n",
    "        print(len(x))\n",
    "    model = LogisticRegression(penalty = 'l1', solver = 'saga', max_iter = 10000)\n",
    "    model.fit(x, y)\n",
    "    #weight_values, weight_indices = nonzero_weights(model)\n",
    "    f_scores, accuracies, wrong_indices, confusion_matrices = k_fold_cv(x, y, k = 5, labels = target_senses)\n",
    "    return {'model': model, \"data\": x, \"labels\": sense_labels, \"acc\": accuracies, \"f1\": f_scores, \n",
    "            'incorrect_indices': wrong_indices, 'sentences': np.asarray(data['original_sentences']),\n",
    "           'confusion_matrices': confusion_matrices}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-8bfa7bf714c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccess_db\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Berkeley/Research/thesis/codebase/core/analysis.py\u001b[0m in \u001b[0;36maccess_db\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mcred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcredentials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCertificate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/wordsense-pilesort-firebase-adminsdk-3ipny-791a81e575.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     firebase_admin.initialize_app(cred, {\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;34m'databaseURL'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'https://wordsense-pilesort.firebaseio.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     })\n\u001b[1;32m     20\u001b[0m     \u001b[0mref\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'polysemy_pilesort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/bert/lib/python3.7/site-packages/firebase_admin/__init__.py\u001b[0m in \u001b[0;36minitialize_app\u001b[0;34m(credential, options, name)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DEFAULT_APP_NAME\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         raise ValueError((\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;34m'The default Firebase app already exists. This means you called '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0;34m'initialize_app() more than once without providing an app name as '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0;34m'the second argument. In most cases you only need to call '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name."
     ]
    }
   ],
   "source": [
    "db = access_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9375\n",
      "[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0] [0 0 0 0 1 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0]\n",
      "[[12  1]\n",
      " [ 1 18]]\n",
      "0.967741935483871\n",
      "[0 0 0 0 0 0 0 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0] [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "[[11  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  1  0]]\n",
      "0.967741935483871\n",
      "[0 0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      "[[15  0  0]\n",
      " [ 0 15  0]\n",
      " [ 0  1  0]]\n",
      "1.0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1]\n",
      "[[19  0]\n",
      " [ 0 12]]\n",
      "0.9354838709677419\n",
      "[0 0 0 0 0 0 0 0 0 0 2 2 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0] [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0]\n",
      "[[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  2  0]]\n"
     ]
    }
   ],
   "source": [
    "model_results = logistic_cv('table.n', sel_senses = [fb_to_local(s) for s in get_senses(db, 'table_n')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[12,  1],\n",
       "        [ 1, 18]]), array([[11,  0,  0],\n",
       "        [ 0, 19,  0],\n",
       "        [ 0,  1,  0]]), array([[15,  0,  0],\n",
       "        [ 0, 15,  0],\n",
       "        [ 0,  1,  0]]), array([[19,  0],\n",
       "        [ 0, 12]]), array([[13,  0,  0],\n",
       "        [ 0, 16,  0],\n",
       "        [ 0,  2,  0]])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['confusion_matrices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot.n\n",
      "['foot.n.01', 'foot.n.02', 'foot.n.03']\n",
      "162\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161]\n",
      "158\n",
      "1.0\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[17  0]\n",
      " [ 0 15]]\n",
      "1.0\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 3 1] [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 3 1]\n",
      "[[12  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  0  1]]\n",
      "0.9375\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 3 3 3 1 1 1] [2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 3 1 1 1 1]\n",
      "[[17  0  0]\n",
      " [ 0 12  0]\n",
      " [ 1  1  1]]\n",
      "1.0\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "[[15  0]\n",
      " [ 0 16]]\n",
      "1.0\n",
      "[2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 3 1 1] [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 3 1 1]\n",
      "[[13  0  0]\n",
      " [ 0 17  0]\n",
      " [ 0  0  1]]\n",
      "plane.n\n",
      "['airplane.n.01', 'plane.n.02', 'plane.n.03']\n",
      "49\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "49\n",
      "0.9\n",
      "[1 2 0 0 0 0 0 0 0 0] [1 0 0 0 0 0 0 0 0 0]\n",
      "[[8 0 0]\n",
      " [0 1 0]\n",
      " [1 0 0]]\n",
      "0.9\n",
      "[1 1 1 1 2 0 0 0 0 0] [1 1 1 1 1 0 0 0 0 0]\n",
      "[[5 0 0]\n",
      " [0 4 0]\n",
      " [0 1 0]]\n",
      "1.0\n",
      "[1 1 1 0 0 0 0 0 0 0] [1 1 1 0 0 0 0 0 0 0]\n",
      "[[7 0]\n",
      " [0 3]]\n",
      "1.0\n",
      "[1 1 1 1 0 0 0 0 0 0] [1 1 1 1 0 0 0 0 0 0]\n",
      "[[6 0]\n",
      " [0 4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/bert/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "[1 1 1 1 0 0 0 0 0] [1 1 1 1 0 0 0 0 0]\n",
      "[[5 0]\n",
      " [0 4]]\n",
      "model.n\n",
      "['model.n.01', 'model.n.02', 'model.n.03']\n",
      "72\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n",
      "65\n",
      "1.0\n",
      "[3 3 1 1 1 2 1 1 1 1 1 1 3] [3 3 1 1 1 2 1 1 1 1 1 1 3]\n",
      "[[9 0 0]\n",
      " [0 1 0]\n",
      " [0 0 3]]\n",
      "0.6153846153846154\n",
      "[3 3 1 1 1 2 2 2 1 1 1 3 1] [1 3 1 1 1 3 2 1 1 1 1 2 2]\n",
      "[[6 1 0]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n",
      "1.0\n",
      "[3 1 1 1 2 2 2 1 1 1 1 1 1] [3 1 1 1 2 2 2 1 1 1 1 1 1]\n",
      "[[9 0 0]\n",
      " [0 3 0]\n",
      " [0 0 1]]\n",
      "0.7692307692307693\n",
      "[1 1 1 1 1 1 2 2 1 1 1 1 3] [1 1 1 1 1 1 3 3 1 1 1 1 1]\n",
      "[[10  0  0]\n",
      " [ 0  0  2]\n",
      " [ 1  0  0]]\n",
      "0.8461538461538461\n",
      "[3 1 1 1 1 2 2 1 1 1 3 3 1] [3 1 1 1 1 2 2 1 1 1 2 2 1]\n",
      "[[8 0 0]\n",
      " [0 2 0]\n",
      " [0 2 1]]\n",
      "table.n\n",
      "['table.n.01', 'table.n.02', 'table.n.03']\n",
      "156\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155]\n",
      "156\n",
      "0.9375\n",
      "[0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1] [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1]\n",
      "[[14  0  0]\n",
      " [ 1 16  0]\n",
      " [ 0  1  0]]\n",
      "0.967741935483871\n",
      "[0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0] [0 0 0 0 0 0 1 0 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      "[[15  1]\n",
      " [ 0 15]]\n",
      "0.967741935483871\n",
      "[0 0 0 0 0 0 0 0 0 0 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0] [0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      "[[11  0  0]\n",
      " [ 0 19  0]\n",
      " [ 0  1  0]]\n",
      "0.9354838709677419\n",
      "[0 0 0 0 0 0 0 0 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0] [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0]\n",
      "[[13  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  2  0]]\n",
      "1.0\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      "[[17  0]\n",
      " [ 0 14]]\n",
      "degree.n\n",
      "['academic_degree.n.01', 'degree.n.01', 'degree.n.02']\n",
      "80\n",
      "[13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "67\n",
      "0.6428571428571429\n",
      "[0 0 0 2 2 1 1 1 1 1 1 1 1 1] [0 0 0 1 1 1 1 1 2 1 2 1 2 1]\n",
      "[[3 0 0]\n",
      " [0 6 3]\n",
      " [0 2 0]]\n",
      "0.6428571428571429\n",
      "[0 0 0 2 2 2 1 1 1 1 1 1 1 1] [0 0 0 2 1 1 1 1 1 1 2 2 1 2]\n",
      "[[3 0 0]\n",
      " [0 5 3]\n",
      " [0 2 1]]\n",
      "0.7692307692307693\n",
      "[0 0 0 2 2 2 1 1 1 1 1 1 1] [0 0 0 1 1 2 2 1 1 1 1 1 1]\n",
      "[[3 0 0]\n",
      " [0 6 1]\n",
      " [0 2 1]]\n",
      "0.8461538461538461\n",
      "[0 0 0 2 2 2 2 1 1 1 1 1 1] [0 0 0 2 2 1 2 2 1 1 1 1 1]\n",
      "[[3 0 0]\n",
      " [0 5 1]\n",
      " [0 1 3]]\n",
      "0.6153846153846154\n",
      "[0 2 2 2 2 2 1 1 1 1 1 1 1] [0 2 1 1 2 1 2 1 1 2 1 1 1]\n",
      "[[1 0 0]\n",
      " [0 5 2]\n",
      " [0 3 2]]\n",
      "right.n\n",
      "['right.n.01', 'right.n.02', 'right.n.04']\n",
      "61\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58]\n",
      "54\n",
      "0.9090909090909091\n",
      "[1 1 1 1 2 0 0 0 0 0 0] [1 1 0 1 2 0 0 0 0 0 0]\n",
      "[[6 0 0]\n",
      " [1 3 0]\n",
      " [0 0 1]]\n",
      "0.8181818181818182\n",
      "[1 2 2 0 0 0 0 0 0 0 0] [1 2 2 0 1 2 0 0 0 0 0]\n",
      "[[6 1 1]\n",
      " [0 1 0]\n",
      " [0 0 2]]\n",
      "1.0\n",
      "[1 1 0 0 0 0 0 0 0 0 0] [1 1 0 0 0 0 0 0 0 0 0]\n",
      "[[9 0]\n",
      " [0 2]]\n",
      "1.0\n",
      "[1 1 0 0 0 0 0 0 0 0 0] [1 1 0 0 0 0 0 0 0 0 0]\n",
      "[[9 0]\n",
      " [0 2]]\n",
      "1.0\n",
      "[1 1 1 2 0 0 0 0 0 0] [1 1 1 2 0 0 0 0 0 0]\n",
      "[[6 0 0]\n",
      " [0 3 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "shared_model_data = {}\n",
    "shared_stats = []\n",
    "incorrect = []\n",
    "shared_words = ['foot.n', 'plane.n', 'model.n', 'table.n', 'degree.n', 'right.n']\n",
    "for w in shared_words:\n",
    "    print(w)\n",
    "    print([fb_to_local(s) for s in get_senses(db, w.replace('.n', \"_n\"))])\n",
    "\n",
    "    model_results = logistic_cv(w, sel_senses = [fb_to_local(s) for s in get_senses(db, w.replace('.n', \"_n\"))])\n",
    "    shared_model_data[w] = model_results\n",
    "    shared_stats.append({\"word\": w, 'avg_f1': np.mean(model_results['f1']),\n",
    "                         'avg_acc': np.mean(model_results['acc'])})    \n",
    "    #incorrect.append(misclassified_sentences(model_results, model_results['incorrect_indices']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>avg_f1</th>\n",
       "      <th>avg_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>foot.n</td>\n",
       "      <td>0.974961</td>\n",
       "      <td>0.981629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>plane.n</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>0.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model.n</td>\n",
       "      <td>0.745612</td>\n",
       "      <td>0.765714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>table.n</td>\n",
       "      <td>0.952730</td>\n",
       "      <td>0.961694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>degree.n</td>\n",
       "      <td>0.682346</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>right.n</td>\n",
       "      <td>0.887547</td>\n",
       "      <td>0.901282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word    avg_f1   avg_acc\n",
       "0    foot.n  0.974961  0.981629\n",
       "1   plane.n  0.922500  0.940000\n",
       "2   model.n  0.745612  0.765714\n",
       "3   table.n  0.952730  0.961694\n",
       "4  degree.n  0.682346  0.700000\n",
       "5   right.n  0.887547  0.901282"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(shared_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['foot.n', 'plane.n', 'model.n', 'table.n', 'degree.n']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert] *",
   "language": "python",
   "name": "conda-env-bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
