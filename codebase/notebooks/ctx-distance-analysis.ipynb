{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.semcor_bert_pipeline import *\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from core.metrics import cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/expt_1_stimuli.csv\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "df['dt_stem'] = df['disambig_token'].apply(lambda t: tokenizer.tokenize(t)[0])\n",
    "a = df[(df['d_s'] == 'd') & (df['early_late'] == 'early')]\n",
    "b = df[(df['d_s'] == 's') & (df['early_late'] == 'early')]\n",
    "c = df[(df['d_s'] == 'd') & (df['early_late'] == 'late')]\n",
    "d = df[(df['d_s'] == 's') & (df['early_late'] == 'early')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_pair_comparison(c1, c2):\n",
    "    #c1 has no \"padding,\" c2 does\n",
    "    model = initialize_model()\n",
    "    early_data = []\n",
    "    late_data = []\n",
    "    for bias in [0, 1]:\n",
    "        for stimulus_index in np.arange(1, 9):\n",
    "            early_row = c1[(c1['bias'] == bias) & (c1['stimulus_index'] == stimulus_index)].T.squeeze()\n",
    "            late_row = c2[(c2['bias'] == bias) & (c2['stimulus_index'] == stimulus_index)].T.squeeze()\n",
    "            early_sent, early_d_tok = early_row['sentence'], early_row['dt_stem']\n",
    "            late_sent, late_d_tok = late_row['sentence'], late_row['dt_stem']\n",
    "            assert early_row['target_token'] == late_row['target_token']\n",
    "            target = early_row['target_token']\n",
    "            early_data.append(target_attns(early_sent, target, early_d_tok, model))\n",
    "            late_data.append(target_attns(late_sent, target, late_d_tok, model))\n",
    "    return early_data, late_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data, c_data = sentence_pair_comparison(a, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_item = lambda l: [i[0] for i in l]\n",
    "second_item = lambda l: [i[1] for i in l]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1, 2, 3, 4, 5, 7},\n",
       " {2, 3, 4, 5, 7},\n",
       " {0, 2, 6},\n",
       " {0, 2, 5, 7},\n",
       " {0, 2, 5, 7},\n",
       " {2},\n",
       " {2, 4, 5},\n",
       " {0, 2, 5, 8},\n",
       " {2, 4, 5},\n",
       " {2, 5},\n",
       " {2, 5},\n",
       " {0, 2, 4, 5, 6, 7},\n",
       " {2, 7},\n",
       " {2, 4, 5, 7},\n",
       " {0, 2, 3, 4, 5, 7, 8},\n",
       " {1, 2, 4, 5, 7}]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_attn, a_embeds = first_item(a_data), second_item(a_data)\n",
    "c_attn, c_embeds = first_item(c_data), second_item(c_data)\n",
    "top_layers(b_attn, d_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0, 1, 2, 3, 4, 5, 7},\n",
       " {2, 3, 4, 5, 7},\n",
       " {0, 2, 6},\n",
       " {0, 2, 5, 7},\n",
       " {0, 2, 5, 7},\n",
       " {2},\n",
       " {2, 4, 5},\n",
       " {0, 2, 5, 8},\n",
       " {2, 4, 5},\n",
       " {2, 5},\n",
       " {2, 5},\n",
       " {0, 2, 4, 5, 6, 7},\n",
       " {2, 7},\n",
       " {2, 4, 5, 7},\n",
       " {0, 2, 3, 4, 5, 7, 8},\n",
       " {1, 2, 4, 5, 7}]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_data, d_data = sentence_pair_comparison(b, d)\n",
    "b_attn, b_embeds = first_item(b_data), second_item(b_data)\n",
    "d_attn, d_embeds = first_item(b_data), second_item(b_data)\n",
    "top_layers(b_attn, d_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_pairwise_cosine_sim(l1, l2):\n",
    "    #l1 and l2 are lists of embeddings\n",
    "    assert len(l1) == len(l2)\n",
    "    n = len(l1)\n",
    "    return np.mean([cosine_sim(l1[i], l2[i]) for i in range(n)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pairs have the same sense but have differing amounts of context between the target token and disambiguating token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pairwise_cosine_sim(b_embeds, d_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96191275"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pairwise_cosine_sim(a_embeds, c_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different senses, same distance to disambiguating token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7927524"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pairwise_cosine_sim(a_embeds, b_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907566"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pairwise_cosine_sim(c_embeds, d_embeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different senses, different distance to disambiguating token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7927524"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pairwise_cosine_sim(a_embeds, d_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7907566"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_pairwise_cosine_sim(c_embeds, b_embeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_layers(early_attn, late_attn):\n",
    "    common_layers = []\n",
    "    for early_dict, late_dict in zip(early_attn, late_attn):\n",
    "        top_token_e = set()\n",
    "        top_token_l = set()\n",
    "        for layer in a_dict.keys():\n",
    "            if np.argsort(-early_dict[layer]['attn_vector'])[0] == early_dict[layer]['hc_token_idx']:\n",
    "                top_token_e.add(layer)\n",
    "            if np.argsort(-late_dict[layer]['attn_vector'])[0] == late_dict[layer]['hc_token_idx']:\n",
    "                top_token_l.add(layer)\n",
    "        common_layers.append(top_token_e.intersection(top_token_l))\n",
    "    return common_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_attns(sentence, target_token, disambig_token, model):\n",
    "    \"\"\"\n",
    "    Inputs: sentence- sentence from experiment\n",
    "    target_token- token we are getting BERT embeddings for\n",
    "    disambig_token- token that reveals information about target_token's sense\n",
    "    model- pretrained BERT model\n",
    "    \n",
    "    Output:\n",
    "    List of ranks of disambig_token in the attention vector for target_token. \n",
    "    Indices in the list correspond to layers.\n",
    "    \"\"\"\n",
    "    indexed_tokens, tokenized_text = preprocess(sentence, target_token)\n",
    "    target_activations, attns = get_model_output(indexed_tokens, model)\n",
    "    attn_dict = process_raw_attentions([attns], [tokenized_text])[0]\n",
    "    #print(tokenized_text[0])\n",
    "    d_tok_idx = tokenized_text[0][1:-1].index(disambig_token) #index of the disambiguating token, removing SEP/CLS\n",
    "    output_dict = {}\n",
    "    target_embeddings = sum_layers(target_activations, -4)\n",
    "    for k in attn_dict.keys():\n",
    "        output_dict[k] = {\"attn_vector\": attn_dict[k][1:-1], \"hc_token_idx\": d_tok_idx}\n",
    "    return output_dict, target_embeddings\n",
    "    #return np.array([np.argwhere(np.argsort(-attn_dict[k]) == d_tok_idx)[0][0] for k in attn_dict])\n",
    "                                                                                                     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bert]",
   "language": "python",
   "name": "conda-env-bert-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
